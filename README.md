#  Machine Learning with Neural Networks
  
  
##  Perceptrons
  
  
The basic unit of work in a neural network is the `perceptron`. A `perceptron` has an associated potential to emit a signal. For convenience the value of the potential is kept between <img src="https://latex.codecogs.com/svg.latex?0"/> and <img src="https://latex.codecogs.com/svg.latex?1"/>. If the potential <img src="https://latex.codecogs.com/svg.latex?p%20=%201"/> the neuron is active, if <img src="https://latex.codecogs.com/svg.latex?p%20=%200"/> the neuron is inactive. We can implement the `perceptron` as a function <img src="https://latex.codecogs.com/svg.latex?P"/> with an array of `activation values` <img src="https://latex.codecogs.com/svg.latex?[a_{1},%20a_{2},%20a_{3},%20...,%20a_{n}]"/> i.e. <img src="https://latex.codecogs.com/svg.latex?a_{1...n}"/>  in it's internal scope. The function parameters are an array of `weight values` <img src="https://latex.codecogs.com/svg.latex?[w_{1},%20w_{2},%20w_{3},%20...,%20w_{n}]"/> or <img src="https://latex.codecogs.com/svg.latex?w_{1...n}"/>. The output then is the signal <img src="https://latex.codecogs.com/svg.latex?p"/>. The values <img src="https://latex.codecogs.com/svg.latex?a_{1...n}"/> and <img src="https://latex.codecogs.com/svg.latex?w_{1...n}"/> are defined as tensors because the types of operations or functions that will be used to manipulate the `perceptrons` comes from a branch of mathematics called [Tensor Analysis](https://en.wikipedia.org/wiki/Tensor_calculus ). Consider the implementation of <img src="https://latex.codecogs.com/svg.latex?P"/> based on the following:
  
* We define tensors <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{A}%20=%20a_{1...n}"/> and <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{W}%20=%20w_{1...n}"/>.
* Multiply tensors <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{A}"/> and <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{W}"/> i.e. <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{C}%20=%20&#x5C;hat{A}%20&#x5C;cdot%20&#x5C;hat{W}"/>.
* The tensor product will be <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{C}%20=%20[a_{1}w_{1},%20a_{2}w_{2},%20a_{3}w_{3},%20...,%20a_{n}w_{n}]"/>.
* Reduce <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{C}"/> to a scalar value by adding it's components.
* The sum of the <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{C}"/> components is <img src="https://latex.codecogs.com/svg.latex?S_{w}%20=%20a_{1}w_{1}%20+%20a_{2}w_{2}%20+%20a_{3}w_{3},%20...,%20a_{n-1}w_{n-1}%20+%20a_{n}w_{n}"/>.
* <img src="https://latex.codecogs.com/svg.latex?{S_{w}}"/> is called a weighted sum  and is represented by <img src="https://latex.codecogs.com/svg.latex?&#x5C;sum_{n=1}^{k}%20a_{n}w_{n}"/> where <img src="https://latex.codecogs.com/svg.latex?k"/> is the number of elements in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{C}"/>.
* <img src="https://latex.codecogs.com/svg.latex?S_{w}"/> determines the strength of the signal emitted by the `perceptron`.
* Capping <img src="https://latex.codecogs.com/svg.latex?S_{w}"/> adds additional control over signal emission and is done by subtracting a bias <img src="https://latex.codecogs.com/svg.latex?b"/> from the sum.
* It is possible for <img src="https://latex.codecogs.com/svg.latex?S_{w}%20-%20b"/> to have a value outside the desire signal strength <img src="https://latex.codecogs.com/svg.latex?0%20&#x5C;geq%20p%20&#x5C;leq%201"/>. For this reason an `activation function` is used to bring <img src="https://latex.codecogs.com/svg.latex?p"/> into the desired range.
* One of the  commonly used `activation functions` is the `sigmoid` <img src="https://latex.codecogs.com/svg.latex?&#x5C;sigma%20(x)%20=%20%20&#x5C;frac%20{&#x5C;mathrm{1}%20}{&#x5C;mathrm{1}%20+%20e^{-x}%20}"/>.
  
In conclusion the implementation of a perceptron is the function <img src="https://latex.codecogs.com/svg.latex?P(&#x5C;hat{W})%20=%20&#x5C;sigma%20(S_{w}%20-%20b)"/>.
  
##  Neural Networks
  
  
A `neural network` is a graph of associated `perceptrons`. `Neural networks` are composed of `neural network layers`. A `neural network layer` is a tensor of `perceptrons`. The `perceptrons` in a `neural network layer` are connected to each other because they are components of a tensor. We can define layer n as <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{n}%20=%20[P_{1},%20P_{2},%20P_{3},%20...%20,P_{n}]"/>. Neural networks have three `layer types input, hidden, and output`. A neural network  may have multiple hidden layers but only one input and output layers. Consider a neural network consisting of the fallowing layers:
  
<div align="center">
<div>
<img with=160 height=160 src="img/simple_nn.svg"/>
</div>
<div>
  
<img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{i}%20=%20[P_{1i}]"/> &nbsp;&nbsp; <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{1h}%20=%20[P_{1h},P_{2h}]"/> &nbsp;&nbsp; <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{o}%20=%20[P_{1o}]"/>
  
</div>
</div>
  
Neural network themselves are tensors. In this case neural network <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{N}%20=%20[&#x5C;hat{L}_{i},%20&#x5C;hat{L}_{1h},%20&#x5C;hat{L}_{o}%20]"/>. `Perceptrons` in a neural network are associated to each other via `function composition`. Consider <img src="https://latex.codecogs.com/svg.latex?P_{1i}"/> it has an internal tensor of `activation values` <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{A}_{i1}%20=%20[a_{1i}]"/>. The number of components in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{A}_{i1}"/> is one. The output of <img src="https://latex.codecogs.com/svg.latex?P_{1i}"/> is a potential <img src="https://latex.codecogs.com/svg.latex?p_{1i}"/>. The key question one must ask at this point is, how are the number of `activation values` in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{1h}"/> associated to the number of `activation values` in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{i}"/>? Here is where the magic happens <img src="https://latex.codecogs.com/svg.latex?p_{1i}"/> becomes the input weight for <img src="https://latex.codecogs.com/svg.latex?P_{1h}"/> and <img src="https://latex.codecogs.com/svg.latex?P_{2h}"/>. This means that <img src="https://latex.codecogs.com/svg.latex?p_{1i}"/> becomes <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{W}_{1i}%20=%20[p_{1i}]"/> a weight value tensor and the input for <img src="https://latex.codecogs.com/svg.latex?P_{1h}"/> and <img src="https://latex.codecogs.com/svg.latex?P_{2h}"/>. This means that the activation values tensor for <img src="https://latex.codecogs.com/svg.latex?P_{1h}"/> is <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{A}_{h1}%20=%20[a_{1h}]"/> a tensor with one component because the input layer consist of only one component <img src="https://latex.codecogs.com/svg.latex?P_{1i}"/>. Is important to notice that `the number of activation values in a layer's perceptrons are determined by the number of perceptrons in the previous layer`.
  
For completeness let's consider the output layer `perceptron` <img src="https://latex.codecogs.com/svg.latex?P_{1o}"/>. Based on our current understanding <img src="https://latex.codecogs.com/svg.latex?P_{1o}"/> has an internal tensor of `activation values` <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{A}_{o1}%20=%20[a_{1o},%20a_{2o}]"/> because <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{1h}"/> has two components <img src="https://latex.codecogs.com/svg.latex?P_{1h}"/> and <img src="https://latex.codecogs.com/svg.latex?P_{2h}"/>. The output for <img src="https://latex.codecogs.com/svg.latex?P_{1h}"/> is a potential <img src="https://latex.codecogs.com/svg.latex?p_{1h}"/> and the output for <img src="https://latex.codecogs.com/svg.latex?P_{2h}"/> is a potential <img src="https://latex.codecogs.com/svg.latex?p_{2h}"/> therefore the weight value tensor is <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{W}_{1h}%20=%20[p_{1h},%20p_{2h}]"/>. The weighted sum for <img src="https://latex.codecogs.com/svg.latex?P_{1o}"/> is <img src="https://latex.codecogs.com/svg.latex?S_{w1o}%20=%20p_{1h}a_{1o}%20+%20p_{2h}a_{2o}"/> and its potential is <img src="https://latex.codecogs.com/svg.latex?p_{1o}%20=%20&#x5C;sigma(S_{w1o}%20-%20b)"/>. Notice how all perceptrons in every layer of the neural network are relaying information i.e. emitting a signal directly or indirectly to each other in a forward direction. The type of neural network where all perceptrons are connected to each other is called dense neural network.
  
  
##  Neural Network Activation
  
  
###  Introduction
  
  
We define a `neural network algorithm` as a function <img src="https://latex.codecogs.com/svg.latex?N"/> that produces an `output` <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{o}"/> in response to an `input` <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{i}"/> and n number of hidden layers <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{1h..nh}"/> i.e. <img src="https://latex.codecogs.com/svg.latex?N(&#x5C;hat{L}_{i},%20&#x5C;hat{L}_{1h..nh})%20=%20&#x5C;hat{L}_{o}"/>. A neural network is a system defined by the following tensor <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{N}%20=%20[&#x5C;hat{L}_{i},%20&#x5C;hat{L}_{1h},%20&#x5C;hat{L}_{2h},%20&#x5C;hat{L}_{3h},...&#x5C;hat{L}_{nh},%20&#x5C;hat{L}_{o}%20]"/>.
In our daily experience we go through time and we have a `state` at each moment in time. Our reality is a series of moments in time. At each moment we can `assess our state` and map any number of metrics to an exact moment in time and persist the resulting information representing our `state`. Our memories are our `state` and we derive knowledge from them. Compare to you or me <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{N}"/> is a very simple system, a moment of time for <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{N}"/> is represented by evaluating <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{1h},%20&#x5C;hat{L}_{2h},%20&#x5C;hat{L}_{3h},...&#x5C;hat{L}_{nh}"/> and <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{o}"/> at a given value of <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}_{i}"/>. We bring <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{N}"/> to life by feeding it `input` and evaluating the `output` of every `perceptron` <img src="https://latex.codecogs.com/svg.latex?P"/> in each neural network layer <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{L}"/>. A neural network is active when <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat{N}"/> is modified by given algorithm <img src="https://latex.codecogs.com/svg.latex?N"/>.
  
###  Back Propagation training using Gradient Descent Algorithm
  
  
Back propagation is a machine learning algorithm. The objective of gradient descent is to find the optimal values for <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20W"/> that will yield <img src="https://latex.codecogs.com/svg.latex?P&#x27;_o"/> in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20N"/> this called training. The algorithm's steps are:
  
1. Identify the unique set of elements in a training set <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20A_{o}"/> call it <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20P_{o}"/> i.e. desire output.
2. Generate an initial set of random numbers corresponding to the number of perceptrons in <img src="https://latex.codecogs.com/svg.latex?L_{i}"/> in the range <img src="https://latex.codecogs.com/svg.latex?0%20&#x5C;geq%20w%20&#x5C;leq%201"/> and use them to construct <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20W_{n}"/> the tensor used to invoke <img src="https://latex.codecogs.com/svg.latex?P"/> for every perceptron in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20L_{i}"/>.
3. Set <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20A_{nh}"/> for all perceptrons in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20L_{nh}"/> and <img src="https://latex.codecogs.com/svg.latex?{&#x5C;hat%20A_{no}}"/> in all perceptrons in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20L_{o}"/> to random numbers in the range <img src="https://latex.codecogs.com/svg.latex?0%20&#x5C;geq%20a%20&#x5C;leq%201"/>. 
4. Iterate over the input data set.
5. At the beginning of each iteration generate <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20A_{ni}"/> for each perceptron in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20L_{i}"/> based on the current input element.
6. Compute <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20W_{n}"/> to invoke <img src="https://latex.codecogs.com/svg.latex?P_{n}"/> for each perceptron in all layers. The output of perceptrons in a previous layer became the input <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20W_{n}"/> for the current layer.
7. At the end of each iteration compute the error between the current output <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20P_{o}"/> and the expected output <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20P&#x27;_{o}"/>. One of the most commonly used error, cost, or loss functions to compare <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20P_{o}"/> vs. <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20P&#x27;_{o}"/> is the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error ) or Loss function <img src="https://latex.codecogs.com/svg.latex?E(p&#x27;_{n},%20p_{n},%20n)%20=%20&#x5C;frac%20{1}{n}%20&#x5C;sum_{i%20=%201}^{n}(p&#x27;_{n}%20-%20p_{n})^2"/>. The error indicates how close the <img src="https://latex.codecogs.com/svg.latex?p&#x27;_n"/> is to <img src="https://latex.codecogs.com/svg.latex?p_n"/>.
8. Compute the rate of change of the error function. The rate of change of a single variable function with one scalar output is called a derivative i.e. <img src="https://latex.codecogs.com/svg.latex?&#x5C;frac{&#x5C;mathrm{dy}%20}{&#x5C;mathrm{d}%20x}"/>. The rate of change of a multi variable function with one scalar output is called the `gradient` i.e <img src="https://latex.codecogs.com/svg.latex?&#x5C;nabla%20(E)"/>.  The `gradient` indicates the direction and magnitude of greatest increase of the error function. In this case the <img src="https://latex.codecogs.com/svg.latex?&#x5C;nabla%20(E)"/> needs to be computed since we are dealing with multi variable tensors.
9. The next step requires <img src="https://latex.codecogs.com/svg.latex?&#x5C;nabla%20(E)"/> to be negative because the objective is to advance towards less error or cost i.e. <img src="https://latex.codecogs.com/svg.latex?g%20=%20-&#x5C;nabla%20(E)"/>. Define `learning rate` <img src="https://latex.codecogs.com/svg.latex?l_{r}"/> a number <img src="https://latex.codecogs.com/svg.latex?0%20&#x5C;geq%20r%20&#x5C;leq%201"/>, used as a factor that determines the magnitude of <img src="https://latex.codecogs.com/svg.latex?&#x5C;Delta%20w"/> in conjunction with <img src="https://latex.codecogs.com/svg.latex?&#x5C;nabla%20(E)"/>. Define `momentum` <img src="https://latex.codecogs.com/svg.latex?m"/> a number between <img src="https://latex.codecogs.com/svg.latex?0%20&#x5C;geq%20r%20&#x5C;leq%201"/>, used as a factor that determines the magnitude of <img src="https://latex.codecogs.com/svg.latex?&#x5C;Delta%20w"/> in conjunction with <img src="https://latex.codecogs.com/svg.latex?&#x5C;nabla%20(E)"/>. The magnitude of <img src="https://latex.codecogs.com/svg.latex?l_{r}"/> will determine how big of a step we take in our search to minimize the error or cost <img src="https://latex.codecogs.com/svg.latex?E"/>. The magnitude of <img src="https://latex.codecogs.com/svg.latex?m"/> will determine how much of an influence the previous values of <img src="https://latex.codecogs.com/svg.latex?w"/> have in our search to minimize the error or cost <img src="https://latex.codecogs.com/svg.latex?E"/>. Compute the scalar values <img src="https://latex.codecogs.com/svg.latex?&#x5C;Delta%20w%20=%20l_{r}g%20+%20mgw_{n-1}"/> by which <img src="https://latex.codecogs.com/svg.latex?w"/> needs change in order to decrease error i.e. bring <img src="https://latex.codecogs.com/svg.latex?p_{o}"/> closer <img src="https://latex.codecogs.com/svg.latex?p&#x27;_{o}"/>.
10. Calculate <img src="https://latex.codecogs.com/svg.latex?&#x5C;Delta%20w"/> in the backward direction to adjust every <img src="https://latex.codecogs.com/svg.latex?w"/> in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20W"/> across all layers in <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20N"/> relative to it's adjacent layer. This step is know as `back propagation`.
11. After iterating over the complete training data set verify that the current error is less or equal to the `error threshold` <img src="https://latex.codecogs.com/svg.latex?E_{t}&#x27;"/> or that the `maximum number of iterations` <img src="https://latex.codecogs.com/svg.latex?I_{max}"/> was reached, if true stop training else continue.
  
Is crucial to understand that <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20W"/> changes as a result of back propagation in the backward direction while <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20A"/> changes as a result of iterating over training data in the forward direction. This means that properly labeled training data is essential in how well <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20N"/> performs when presented with new information. When practicing machine learning you will be presented with the opportunity to adjust what are called hyper parameters some of them are:
  
* <img src="https://latex.codecogs.com/svg.latex?E_{t}"/> error threshold.
* <img src="https://latex.codecogs.com/svg.latex?I_{max}"/> expected number of iterations.
* <img src="https://latex.codecogs.com/svg.latex?l_{r}"/> learning rate.
* <img src="https://latex.codecogs.com/svg.latex?m"/> momentum.
  
Persisting a trained <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20N"/> is essential, notice that all that needs to be persisted is <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20W"/>, associated biases, and all the hyper parameters used during training.
  
###  Training Data
  
  
The process of preparing training data sets is challenging and can vary depending on what you are trying to accomplish. Imagine we have a set of data representing the horse power <img src="https://latex.codecogs.com/svg.latex?hp"/>, and the miles per gallon <img src="https://latex.codecogs.com/svg.latex?mpg"/> of a model <img src="https://latex.codecogs.com/svg.latex?m"/>. The array <img src="https://latex.codecogs.com/svg.latex?raw_{n}%20=%20[m_{n},%20hp_{n},%20mpg_{n}]"/> represents an element in our raw dataset. Our objective is to determine if there is a relationship between <img src="https://latex.codecogs.com/svg.latex?hp"/> and <img src="https://latex.codecogs.com/svg.latex?mpg"/> and to design a neural network <img src="https://latex.codecogs.com/svg.latex?&#x5C;hat%20R"/> that will help us predict the <img src="https://latex.codecogs.com/svg.latex?mpg"/> given <img src="https://latex.codecogs.com/svg.latex?hp"/>. To prepare the data for consumption we need understand what are the inputs and outputs for our model. Since our intent is to predict <img src="https://latex.codecogs.com/svg.latex?mpg"/> in relationship to <img src="https://latex.codecogs.com/svg.latex?hp"/> regardless of the model, then our training data becomes <img src="https://latex.codecogs.com/svg.latex?data_{n}%20=%20[hp_{n},%20mpg_{n}]"/>. The last step in the process is data normalization, and is usually accomplished by [min-max feature scaling](https://en.wikipedia.org/wiki/Feature_scaling ). The function for `min-max feature scaling` is <img src="https://latex.codecogs.com/svg.latex?normalize(a_{n},%20a_{min},%20a_{max})%20=%20(&#x5C;frac{a_{n}%20-%20a_{imin}}{a_{max}%20-%20a_{min}})"/> where <img src="https://latex.codecogs.com/svg.latex?a_{n}"/> is any value, <img src="https://latex.codecogs.com/svg.latex?a_{max}"/> is the maximum, and <img src="https://latex.codecogs.com/svg.latex?a_{min}"/> is the minimum in the array <img src="https://latex.codecogs.com/svg.latex?[a_{1},%20a_{2},%20a_{3}...a_{n}]"/>. Normalization assures that the value <img src="https://latex.codecogs.com/svg.latex?a_{n}"/> is always within the range <img src="https://latex.codecogs.com/svg.latex?0%20&#x5C;leq%20a_{ni}%20&#x5C;geq%201"/>. In our case study <img src="https://latex.codecogs.com/svg.latex?a_{ni}%20=%20normalize(hp_{n},%20hp_{min},%20hp_{max})"/> and the expected output <img src="https://latex.codecogs.com/svg.latex?a_{no}%20=%20normalize(mpg_{n},%20mpg_{min},%20mpg_{max})"/>. In a use case where the neural network objective is to classify information the output is the set of unique labels associated with the data. Labeling data is absolutely necessary and it can take a tremendous amount of time.
  
  
  
  
  